# -*- coding: utf-8 -*-
"""Layer_8_ML_Project (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXTCq8UbHORHyHapmgD20HHYdOXr15RD
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm, metrics
import matplotlib.pyplot as plt
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

# Constants
LABELS = ['label_1', 'label_2', 'label_3', 'label_4']
FEATURES = ['feature_'+ str(i) for i in range(1,769)]

trainDf = pd.read_csv("./train.csv")
trainDf.head()

trainDf.describe()

validDf = pd.read_csv("./valid.csv")
validDf.head()

testDf = pd.read_csv("./test.csv")
testDf.head()

"""## Handling Missing Data"""

# Check for missing values
trainDf.isnull().sum()

"""Label_2 has 480 rows with missing values. Those will be neglected."""

# Creating histograms for all the labels
fig, axes = plt.subplots(1, len(LABELS), figsize=(16, 4))
for i, label in enumerate(LABELS):
    # Histogram
    if(label == 'label_3'):
      axes[i].hist(trainDf[label], bins=2, edgecolor='black')
    else:
      axes[i].hist(trainDf[label], bins=10, edgecolor='black')
    axes[i].set_xlabel(label)
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'Histogram for {label}')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""## Scaling

Scaling the data using the standard scaler. This is used to transform and scale features in this dataset to have a mean of 0 and a standard deviation of 1, making them suitable to train the machine learning models

Train and validation data for all 4 labels will be scaled and stored in a dictionary for future reference.
"""

data_dict = {"label_1" : {},"label_2" : {},"label_3" : {},"label_4" : {}}
tr_df = trainDf
vl_df = validDf
tst_df = testDf

for label in LABELS:
    if (label == 'label_2'):
        tr_df = trainDf[trainDf['label_2'].notna()]
        vl_df = validDf[validDf['label_2'].notna()]
    scaler = StandardScaler()
    data_dict[label]['X_train'] = pd.DataFrame(scaler.fit_transform(tr_df.drop(LABELS, axis = 1)), columns = FEATURES)
    data_dict[label]['Y_train'] = tr_df[label]
    data_dict[label]['X_valid'] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis = 1)), columns = FEATURES)
    data_dict[label]['Y_valid'] = vl_df[label]
    data_dict['X_test'] = pd.DataFrame(scaler.transform(tst_df.drop("ID", axis = 1)), columns = FEATURES)

"""The dataset will be trained using an svm initially without any hyper parameter tuning. The accuracy values will be observed here"""

initial_model = svm.SVC(kernel = 'rbf', random_state = 42)
for label in LABELS:
    initial_model.fit(data_dict[label]['X_train'], data_dict[label]['Y_train'])
    y_pred = initial_model.predict(data_dict[label]['X_valid'])
    print(label,"_accuracy : ",metrics.accuracy_score(data_dict[label]['Y_valid'] ,y_pred))

"""Predicting label 3 of test data using the initial model since it's accuracy is 100%"""

initial_label_3_model = svm.SVC(kernel='rbf',random_state=42)
initial_label_3_model.fit(data_dict['label_3']['X_train'], data_dict['label_3']['Y_train'])
label_3_y_test_pred = initial_label_3_model.predict(data_dict['X_test'])
df = pd.DataFrame(label_3_y_test_pred, columns=['label_3'])
print(df)
df.to_csv('label_3_initial_svm.csv', index=False)

"""Note : This didn't improve the performance in kaggle

### Hyperparameter Tuning for all 4 lables
"""

model = svm.SVC(kernel='rbf', random_state = 42)
# Define the hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'gamma' : ['scale','auto']
}

# Create a HalvingGridSearchCV instance
search = HalvingGridSearchCV(model, param_grid, cv=5, factor=3, verbose=1, scoring='accuracy',n_jobs=7)

# Fit the search to the training data
for label in LABELS:
    search.fit(data_dict[label]['X_train'], data_dict[label]['Y_train'])

    # Get the best hyperparameters and best model
    best_params = search.best_params_
    best_model = search.best_estimator_

    # Evaluate the best model on the validation set
    accuracy = best_model.score(data_dict[label]['X_valid'], data_dict[label]['Y_valid'])

    print(label,"_Best C:", best_params['C'])
    print(label,"_Best Gamma:", best_params['gamma'])
    print(label,"_Validation Accuracy:", accuracy)

"""After obtaining the best parameter values for each label, a separate model will be trained for each label and the test labels will be predicted"""

label_1_model = svm.SVC(kernel='rbf', random_state = 42, C= 100, gamma= 'auto')
label_1_model.fit(data_dict['label_1']['X_train'], data_dict['label_1']['Y_train'])
y_test_pred = label_1_model.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred, columns=['label_1'])
print(df)
df.to_csv('label_1_halving_svm.csv', index=False)

label_2_model = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'auto')
label_2_model.fit(data_dict['label_2']['X_train'], data_dict['label_2']['Y_train'])
y_test_pred = label_2_model.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred, columns=['label_2'])
print(df)
df.to_csv('label_2_halving_svm.csv', index=False)

label_3_model = svm.SVC(kernel='rbf', random_state = 42, C= 100, gamma= 'auto')
label_3_model.fit(data_dict['label_3']['X_train'], data_dict['label_3']['Y_train'])
y_test_pred = label_3_model.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred, columns=['label_3'])
print(df)
df.to_csv('label_3_halving_svm.csv', index=False)

label_4_model = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'scale')
label_4_model.fit(data_dict['label_4']['X_train'], data_dict['label_4']['Y_train'])
y_test_pred = label_4_model.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred, columns=['label_4'])
print(df)
df.to_csv('label_4_halving_svm.csv', index=False)

"""### More on hyperparameter tuning

Hyperparameter tuning is continued with adding more parameter values to the param grid
"""

model = svm.SVC(random_state = 42)
# Define the hyperparameter grid

param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # Expanded values for C
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma': ['scale','auto'],
    'class_weight': [None, 'balanced'],
}

# Create a HalvingGridSearchCV instance
search = HalvingGridSearchCV(model, param_grid, cv=5, factor=3, verbose=1, scoring='accuracy',n_jobs=7)

# Fit the search to the training data
for label in LABELS:
    search.fit(data_dict[label]['X_train'], data_dict[label]['Y_train'])

    # Get the best hyperparameters and best model
    best_params = search.best_params_
    best_model = search.best_estimator_

    # Evaluate the best model on the validation set
    accuracy = best_model.score(data_dict[label]['X_valid'], data_dict[label]['Y_valid'])

    print(label,"_Best C:", best_params['C'])
    print(label,"_Best Gamma:", best_params['gamma'])
    print(label,"_Best Kernel:", best_params['kernel'])
    print(label,"_Best class_weight:", best_params['class_weight'])
    print(label,"_Validation Accuracy:", accuracy)

"""Each label is predicted in the test data with the obtained tuned parameters"""

label_1_model_more_hp = svm.SVC(kernel='linear', random_state = 42, C= 0.01, gamma= 'scale',class_weight=None)
label_1_model_more_hp.fit(data_dict['label_1']['X_train'], data_dict['label_1']['Y_train'])
y_test_pred_more_hp = label_1_model_more_hp.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred_more_hp, columns=['label_1'])
print(df)
df.to_csv('label_1_halving-more_svm.csv', index=False)

label_2_model_more_hp = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'auto',class_weight=None)
label_2_model_more_hp.fit(data_dict['label_2']['X_train'], data_dict['label_2']['Y_train'])
y_test_pred_more_hp = label_2_model_more_hp.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred_more_hp, columns=['label_2'])
print(df)
df.to_csv('label_2_halving-more_svm.csv', index=False)

label_3_model_more_hp = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'auto',class_weight='balanced')
label_3_model_more_hp.fit(data_dict['label_3']['X_train'], data_dict['label_3']['Y_train'])
y_test_pred_more_hp = label_3_model_more_hp.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred_more_hp, columns=['label_3'])
print(df)
df.to_csv('label_3_halving-more_svm.csv', index=False)

label_4_model_more_hp = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'scale',class_weight=None)
label_4_model_more_hp.fit(data_dict['label_4']['X_train'], data_dict['label_4']['Y_train'])
y_test_pred_more_hp = label_4_model_more_hp.predict(data_dict['X_test'])
df = pd.DataFrame(y_test_pred_more_hp, columns=['label_4'])
print(df)
df.to_csv('label_4_halving-more_svm.csv', index=False)

"""### Feature Engineering

PCA will be applied for all labels
"""

from sklearn.decomposition import PCA

pca = PCA(n_components= 0.95, svd_solver= 'full')
for label in LABELS:
    pca.fit(data_dict[label]['X_train'])
    data_dict[label]['X_train_after_pca'] = pd.DataFrame(pca.transform(data_dict[label]['X_train']))
    data_dict[label]['X_valid_after_pca'] = pd.DataFrame(pca.transform(data_dict[label]['X_valid']))
    data_dict[label]['X_test_after_pca'] = pd.DataFrame(pca.transform(data_dict['X_test']))
    print(data_dict[label]['X_test_after_pca'].shape)

"""Hyper parameter tuning after PCA"""

model_after_pca = svm.SVC(kernel='rbf', random_state = 42)
# Define the hyperparameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'gamma' : ['scale','auto']
}

# Create a HalvingGridSearchCV instance
search = HalvingGridSearchCV(model_after_pca, param_grid, cv=5, factor=3, verbose=1, scoring='accuracy',n_jobs=7)

# Fit the search to your training data
for label in LABELS:
    search.fit(data_dict[label]['X_train_after_pca'], data_dict[label]['Y_train'])

    # Get the best hyperparameters and best model
    best_params = search.best_params_
    best_model = search.best_estimator_

    # Evaluate the best model on the validation set
    accuracy = best_model.score(data_dict[label]['X_valid_after_pca'], data_dict[label]['Y_valid'])

    print(label,"_Best C:", best_params['C'])
    print(label,"_Best Gamma:", best_params['gamma'])
    print(label,"_Validation Accuracy:", accuracy)

"""4 new models will be trained for the feature engineered data sets and the predictions will be obtained"""

label_1_model_after_pca = svm.SVC(kernel='rbf', random_state = 42, C= 100, gamma= 'scale')
label_1_model_after_pca.fit(data_dict['label_1']['X_train_after_pca'], data_dict['label_1']['Y_train'])
y_test_pred_after_pca = label_1_model_after_pca.predict(data_dict['label_1']['X_test_after_pca'])
df = pd.DataFrame(y_test_pred_after_pca, columns=['label_1'])
print(df)
df.to_csv('label_1_pca_halving_svm.csv', index=False)

label_2_model_after_pca = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'scale')
label_2_model_after_pca.fit(data_dict['label_2']['X_train_after_pca'], data_dict['label_2']['Y_train'])
y_test_pred_after_pca = label_2_model_after_pca.predict(data_dict['label_2']['X_test_after_pca'])
df = pd.DataFrame(y_test_pred_after_pca, columns=['label_2'])
print(df)
df.to_csv('label_2_pca_halving_svm.csv', index=False)

label_3_model_after_pca = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'scale')
label_3_model_after_pca.fit(data_dict['label_3']['X_train_after_pca'], data_dict['label_3']['Y_train'])
y_test_pred_after_pca = label_3_model_after_pca.predict(data_dict['label_3']['X_test_after_pca'])
df = pd.DataFrame(y_test_pred_after_pca, columns=['label_3'])
print(df)
df.to_csv('label_3_pca_halving_svm.csv', index=False)

label_4_model_after_pca = svm.SVC(kernel='rbf', random_state = 42, C= 10, gamma= 'scale')
label_4_model_after_pca.fit(data_dict['label_4']['X_train_after_pca'], data_dict['label_4']['Y_train'])
y_test_pred_after_pca = label_4_model_after_pca.predict(data_dict['label_4']['X_test_after_pca'])
df = pd.DataFrame(y_test_pred_after_pca, columns=['label_4'])
print(df)
df.to_csv('label_4_pca_halving_svm.csv', index=False)